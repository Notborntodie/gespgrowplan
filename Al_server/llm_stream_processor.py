import json
import os
import requests
from typing import Dict, Any, List, Generator, AsyncGenerator
import re
import asyncio

class LLMStreamProcessor:
    """
    çœŸæ­£çš„æµå¼LLMå¤„ç†å™¨ï¼Œæ”¯æŒå®æ—¶æµå¼è¾“å‡ºé¢˜ç›®
    """
    
    def __init__(self, api_key: str = None, max_tokens: int = 16000, model: str = None):
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        self.api_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
        self.model = model or os.getenv("LLM_MODEL", "qwen-plus-latest")
        # å¢åŠ é»˜è®¤max_tokensï¼Œç¡®ä¿é•¿æ–‡æœ¬ä¹Ÿèƒ½å®Œæ•´è¾“å‡º
        self.max_tokens = max_tokens if max_tokens else 32000
    
    def create_question_prompt(self, pdf_text: str, expected_questions: int = None) -> str:
        """åˆ›å»ºæå–é¢˜ç›®çš„prompt"""
        calibration_text = ""
        if expected_questions is not None:
            calibration_text = f"""

âš ï¸âš ï¸âš ï¸ æå…¶é‡è¦çš„è¦æ±‚ âš ï¸âš ï¸âš ï¸ï¼š
- é¢„æœŸé¢˜ç›®æ•°é‡ï¼š{expected_questions} ä¸ª
- ä½ å¿…é¡»ç”Ÿæˆå®Œæ•´çš„ {expected_questions} ä¸ªé¢˜ç›®ï¼Œä¸€ä¸ªéƒ½ä¸èƒ½å°‘ï¼
- è¯·ä»”ç»†æ£€æŸ¥æ•´ä¸ªPDFæ–‡æœ¬ï¼Œç¡®ä¿æå–æ‰€æœ‰é¢˜ç›®
- å¦‚æœå½“å‰æå–çš„é¢˜ç›®æ•°é‡å°‘äº {expected_questions} ä¸ªï¼Œè¯·ç»§ç»­ä»”ç»†æ£€æŸ¥æ–‡æœ¬å†…å®¹
- ä¸è¦é—æ¼ä»»ä½•é¢˜ç›®ï¼Œå³ä½¿é¢˜ç›®æ ¼å¼ç•¥æœ‰ä¸åŒ
- æ¯ä¸ªé¢˜ç›®éƒ½è¦å®Œæ•´æå–ï¼ŒåŒ…æ‹¬é¢˜ç›®æ–‡æœ¬ã€é€‰é¡¹ã€æ­£ç¡®ç­”æ¡ˆå’Œè§£é‡Š
- åœ¨ç”Ÿæˆå®Œæ‰€æœ‰ {expected_questions} ä¸ªé¢˜ç›®ä¹‹å‰ï¼Œä¸è¦åœæ­¢è¾“å‡º
- å¦‚æœé‡åˆ°å›°éš¾ï¼Œè¯·é‡æ–°æ£€æŸ¥PDFæ–‡æœ¬ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼ä»»ä½•é¢˜ç›®"""
        
        return f"""è¯·å°†ä»¥ä¸‹PDFæ–‡æœ¬ä¸­çš„é¢˜ç›®è½¬æ¢ä¸ºæ ‡å‡†JSONæ ¼å¼ã€‚

è¦æ±‚ï¼š
1. åˆ†question_typeä¸¤ç±»é¢˜ç›®ï¼šcodeå’Œtext
2. å¦‚æœé¢˜ç›®æœ‰ä»£ç ï¼Œåˆ™æ˜¯codeç±»å‹ï¼Œåä¹‹text
3. å¦‚æœæ˜¯codeç±»å‹ï¼Œåˆ™æœ‰question_codeå­—æ®µï¼Œå¦‚æœæ˜¯textç±»å‹ï¼Œåˆ™question_codeæ˜¯ç©ºå­—ç¬¦ä¸²
4. æ¯ä¸ªé¢˜ç›®åŒ…å«ï¼šquestion_text, question_type, question_code, correct_answer, explanation, level, difficulty, options
5. levelè¡¨ç¤ºéš¾åº¦ç­‰çº§ï¼ˆ1-5ï¼‰ï¼Œdifficultyè¡¨ç¤ºéš¾åº¦æè¿°ï¼ˆeasy/medium/hardï¼‰{calibration_text}

é‡è¦æ ¼å¼è¦æ±‚ï¼š
- æ¯ä¸ªé¢˜ç›®ä¹‹é—´ç”¨ "---QUESTION_SEPARATOR---" åˆ†éš”
- æ¯å®Œæˆä¸€ä¸ªé¢˜ç›®å°±ç«‹å³è¾“å‡ºï¼Œä¸è¦ç­‰å¾…æ‰€æœ‰é¢˜ç›®å®Œæˆ
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºæ¯ä¸ªé¢˜ç›®

ä»£ç å¤„ç†è¦æ±‚ï¼š
- å¯¹äºcodeç±»å‹çš„é¢˜ç›®ï¼Œquestion_codeå­—æ®µä¸­çš„ä»£ç éœ€è¦ï¼š
  1. ç§»é™¤è¡Œå·ï¼ˆå¦‚"1 cin >> a;" â†’ "cin >> a;"ï¼‰
  2. æ·»åŠ æ­£ç¡®çš„ç¼©è¿›ï¼ˆæ ¹æ®ä»£ç ç»“æ„æ·»åŠ é€‚å½“çš„ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼‰
  3. ä¿æŒä»£ç çš„å®Œæ•´æ€§å’Œå¯è¯»æ€§
  4. ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®

é¢˜ç›®æ–‡æœ¬å¤„ç†è¦æ±‚ï¼š
- question_textå­—æ®µä¸­ä¸è¦åŒ…å«é¢˜ç›®ç¼–å·ï¼ˆå¦‚"ç¬¬1é¢˜"ã€"ç¬¬2é¢˜"ç­‰ï¼‰
- åªä¿ç•™é¢˜ç›®çš„å®é™…å†…å®¹ï¼Œå»é™¤ç¼–å·å‰ç¼€
- ä¿æŒé¢˜ç›®çš„å®Œæ•´æ€§å’Œæ¸…æ™°åº¦

PDFæ–‡æœ¬ï¼š
{pdf_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
{{
  "question_text": "é¢˜ç›®æ–‡æœ¬ï¼ˆä¸åŒ…å«é¢˜ç›®ç¼–å·ï¼‰",
  "question_type": "codeæˆ–text",
  "question_code": "ä»£ç å†…å®¹ï¼ˆç§»é™¤è¡Œå·ï¼Œæ·»åŠ æ­£ç¡®ç¼©è¿›ï¼‰æˆ–ç©ºå­—ç¬¦ä¸²",
  "correct_answer": "æ­£ç¡®ç­”æ¡ˆæ ‡ç­¾",
  "explanation": "è§£é‡Šè¯´æ˜",
  "level": éš¾åº¦ç­‰çº§,
  "difficulty": "éš¾åº¦æè¿°",
  "options": [
    {{"label": "A", "value": "A", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "B", "value": "B", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "C", "value": "C", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "D", "value": "D", "text": "é€‰é¡¹å†…å®¹"}}
  ]
}}
---QUESTION_SEPARATOR---
{{
  "question_text": "ä¸‹ä¸€ä¸ªé¢˜ç›®æ–‡æœ¬ï¼ˆä¸åŒ…å«é¢˜ç›®ç¼–å·ï¼‰",
  "question_type": "codeæˆ–text",
  "question_code": "ä»£ç å†…å®¹ï¼ˆç§»é™¤è¡Œå·ï¼Œæ·»åŠ æ­£ç¡®ç¼©è¿›ï¼‰æˆ–ç©ºå­—ç¬¦ä¸²",
  "correct_answer": "æ­£ç¡®ç­”æ¡ˆæ ‡ç­¾",
  "explanation": "è§£é‡Šè¯´æ˜",
  "level": éš¾åº¦ç­‰çº§,
  "difficulty": "éš¾åº¦æè¿°",
  "options": [
    {{"label": "A", "value": "A", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "B", "value": "B", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "C", "value": "C", "text": "é€‰é¡¹å†…å®¹"}},
    {{"label": "D", "value": "D", "text": "é€‰é¡¹å†…å®¹"}}
  ]
}}
---QUESTION_SEPARATOR---"""
    
    def call_api_stream(self, prompt: str) -> Generator[str, None, None]:
        """è°ƒç”¨DashScope APIå¹¶è¿”å›æµå¼å“åº”"""
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # æ ¹æ®prompté•¿åº¦åŠ¨æ€è°ƒæ•´max_tokensï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è¾“å‡ºç©ºé—´
            # ä¼°ç®—ï¼šæ¯ä¸ªé¢˜ç›®å¤§çº¦éœ€è¦500-1000 tokensï¼ŒåŠ ä¸Špromptæœ¬èº«
            estimated_output_tokens = len(prompt) // 2  # ç²—ç•¥ä¼°ç®—è¾“å‡ºtokenæ•°
            dynamic_max_tokens = max(self.max_tokens, estimated_output_tokens + 10000)
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¢˜ç›®è§£æåŠ©æ‰‹ã€‚ä½ å¿…é¡»å®Œæ•´æå–æ‰€æœ‰é¢˜ç›®ï¼Œä¸èƒ½é—æ¼ä»»ä½•ä¸€é“é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": min(dynamic_max_tokens, 32000),  # è®¾ç½®æœ€å¤§è¾“å‡ºtokenæ•°ï¼Œä½†ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }
            
            response = requests.post(
                self.api_url, 
                headers=headers, 
                json=data, 
                timeout=120,
                stream=True
            )
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            chunk_data = json.loads(data_str)
                            if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                delta = chunk_data['choices'][0].get('delta', {})
                                if 'content' in delta:
                                    yield delta['content']
                        except json.JSONDecodeError:
                            continue
            
        except Exception as e:
            raise Exception(f"è°ƒç”¨DashScope APIå¤±è´¥: {str(e)}")
    
    async def call_api_stream_async(self, prompt: str) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥è°ƒç”¨DashScope APIå¹¶è¿”å›æµå¼å“åº”"""
        try:
            import aiohttp
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # æ ¹æ®prompté•¿åº¦åŠ¨æ€è°ƒæ•´max_tokensï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è¾“å‡ºç©ºé—´
            # ä¼°ç®—ï¼šæ¯ä¸ªé¢˜ç›®å¤§çº¦éœ€è¦500-1000 tokensï¼ŒåŠ ä¸Špromptæœ¬èº«
            estimated_output_tokens = len(prompt) // 2  # ç²—ç•¥ä¼°ç®—è¾“å‡ºtokenæ•°
            dynamic_max_tokens = max(self.max_tokens, estimated_output_tokens + 10000)
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¢˜ç›®è§£æåŠ©æ‰‹ã€‚ä½ å¿…é¡»å®Œæ•´æå–æ‰€æœ‰é¢˜ç›®ï¼Œä¸èƒ½é—æ¼ä»»ä½•ä¸€é“é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": min(dynamic_max_tokens, 32000),  # è®¾ç½®æœ€å¤§è¾“å‡ºtokenæ•°ï¼Œä½†ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url, 
                    headers=headers, 
                    json=data, 
                    timeout=120
                ) as response:
                    response.raise_for_status()
                    
                    async for line in response.content:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                            if data_str.strip() == '[DONE]':
                                break
                            try:
                                chunk_data = json.loads(data_str)
                                if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                    delta = chunk_data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
            
        except Exception as e:
            raise Exception(f"è°ƒç”¨DashScope APIå¤±è´¥: {str(e)}")
    
    def parse_streaming_json(self, stream_generator: Generator[str, None, None]) -> Generator[Dict[str, Any], None, None]:
        """è§£ææµå¼JSONå“åº”ï¼Œä½¿ç”¨åˆ†å‰²ç¬¦å®æ—¶æå–å®Œæ•´é¢˜ç›®"""
        buffer = ""
        separator = "---QUESTION_SEPARATOR---"
        
        for chunk in stream_generator:
            buffer += chunk
            
            # æŸ¥æ‰¾åˆ†å‰²ç¬¦ï¼Œæå–å®Œæ•´çš„é¢˜ç›®
            while separator in buffer:
                # æ‰¾åˆ°åˆ†å‰²ç¬¦çš„ä½ç½®
                separator_pos = buffer.find(separator)
                
                # æå–åˆ†å‰²ç¬¦ä¹‹å‰çš„å†…å®¹ä½œä¸ºé¢˜ç›®JSON
                question_json = buffer[:separator_pos].strip()
                
                # ç§»é™¤å·²å¤„ç†çš„éƒ¨åˆ†
                buffer = buffer[separator_pos + len(separator):].strip()
                
                # å°è¯•è§£æé¢˜ç›®JSON
                if question_json:
                    try:
                        question_obj = json.loads(question_json)
                        if self.is_valid_question(question_obj):
                            yield question_obj
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯: {e}")
                        print(f"é—®é¢˜JSON: {question_json[:200]}...")
                        continue
        
        # æµç»“æŸåï¼Œå¤„ç†bufferä¸­å‰©ä½™çš„æœ€åä¸€ä¸ªé¢˜ç›®ï¼ˆå¯èƒ½æ²¡æœ‰åˆ†éš”ç¬¦ï¼‰
        if buffer.strip():
            # å°è¯•æå–å¯èƒ½çš„JSONå¯¹è±¡
            buffer = buffer.strip()
            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            try:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªbuffer
                question_obj = json.loads(buffer)
                if self.is_valid_question(question_obj):
                    yield question_obj
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                # æŸ¥æ‰¾æœ€åä¸€ä¸ª { å’Œå¯¹åº”çš„ }
                last_open = buffer.rfind('{')
                if last_open != -1:
                    # å°è¯•ä»æœ€åä¸€ä¸ª { å¼€å§‹è§£æ
                    for i in range(len(buffer), last_open, -1):
                        try:
                            potential_json = buffer[last_open:i]
                            question_obj = json.loads(potential_json)
                            if self.is_valid_question(question_obj):
                                yield question_obj
                                break
                        except json.JSONDecodeError:
                            continue
    
    async def parse_streaming_json_async(self, stream_generator: AsyncGenerator[str, None]) -> AsyncGenerator[Dict[str, Any], None]:
        """å¼‚æ­¥è§£ææµå¼JSONå“åº”ï¼Œä½¿ç”¨åˆ†å‰²ç¬¦å®æ—¶æå–å®Œæ•´é¢˜ç›®"""
        buffer = ""
        separator = "---QUESTION_SEPARATOR---"
        
        async for chunk in stream_generator:
            buffer += chunk
            
            # æŸ¥æ‰¾åˆ†å‰²ç¬¦ï¼Œæå–å®Œæ•´çš„é¢˜ç›®
            while separator in buffer:
                # æ‰¾åˆ°åˆ†å‰²ç¬¦çš„ä½ç½®
                separator_pos = buffer.find(separator)
                
                # æå–åˆ†å‰²ç¬¦ä¹‹å‰çš„å†…å®¹ä½œä¸ºé¢˜ç›®JSON
                question_json = buffer[:separator_pos].strip()
                
                # ç§»é™¤å·²å¤„ç†çš„éƒ¨åˆ†
                buffer = buffer[separator_pos + len(separator):].strip()
                
                # å°è¯•è§£æé¢˜ç›®JSON
                if question_json:
                    try:
                        question_obj = json.loads(question_json)
                        if self.is_valid_question(question_obj):
                            yield question_obj
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯: {e}")
                        print(f"é—®é¢˜JSON: {question_json[:200]}...")
                        continue
        
        # æµç»“æŸåï¼Œå¤„ç†bufferä¸­å‰©ä½™çš„æœ€åä¸€ä¸ªé¢˜ç›®ï¼ˆå¯èƒ½æ²¡æœ‰åˆ†éš”ç¬¦ï¼‰
        if buffer.strip():
            # å°è¯•æå–å¯èƒ½çš„JSONå¯¹è±¡
            buffer = buffer.strip()
            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            try:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªbuffer
                question_obj = json.loads(buffer)
                if self.is_valid_question(question_obj):
                    yield question_obj
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                # æŸ¥æ‰¾æœ€åä¸€ä¸ª { å’Œå¯¹åº”çš„ }
                last_open = buffer.rfind('{')
                if last_open != -1:
                    # å°è¯•ä»æœ€åä¸€ä¸ª { å¼€å§‹è§£æ
                    for i in range(len(buffer), last_open, -1):
                        try:
                            potential_json = buffer[last_open:i]
                            question_obj = json.loads(potential_json)
                            if self.is_valid_question(question_obj):
                                yield question_obj
                                break
                        except json.JSONDecodeError:
                            continue
    
    def is_valid_question(self, obj: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„é¢˜ç›®å¯¹è±¡"""
        required_fields = ['question_text', 'question_type', 'correct_answer', 'options']
        return all(field in obj for field in required_fields)
    
    def split_pdf_text_intelligently(self, pdf_text: str, max_chunk_size: int = 3000) -> List[str]:
        """
        æ™ºèƒ½åˆ†å‰²PDFæ–‡æœ¬ï¼Œä¿æŒé¢˜ç›®å®Œæ•´æ€§
        åŸºäºpdfplumberæå–çš„ä¼˜åŒ–æ ¼å¼è¿›è¡Œåˆ†å‰²
        
        Args:
            pdf_text: PDFæ–‡æœ¬å†…å®¹
            max_chunk_size: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        if len(pdf_text) <= max_chunk_size:
            return [pdf_text]
        
        chunks = []
        lines = pdf_text.split('\n')
        
        # è¯†åˆ«é¢˜ç›®è¾¹ç•Œ
        question_boundaries = []
        for i, line in enumerate(lines):
            # åŒ¹é…é¢˜ç›®ç¼–å·æ¨¡å¼
            if re.match(r'ç¬¬\s*\d+\s*é¢˜', line.strip()):
                question_boundaries.append(i)
            # åŒ¹é…ç« èŠ‚åˆ†éš”
            elif re.match(r'\d+\s*(å•é€‰é¢˜|åˆ¤æ–­é¢˜|ç¼–ç¨‹é¢˜)', line.strip()):
                question_boundaries.append(i)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢˜ç›®è¾¹ç•Œï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
        if not question_boundaries:
            return self._split_by_size_fallback(pdf_text, max_chunk_size)
        
        # æŒ‰é¢˜ç›®è¾¹ç•Œåˆ†å‰²
        current_chunk = ""
        current_size = 0
        
        for i, line in enumerate(lines):
            line_with_newline = line + '\n'
            line_size = len(line_with_newline)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢˜ç›®è¾¹ç•Œä¸”å½“å‰å—å·²ç»æœ‰ä¸€å®šå¤§å°
            if (i in question_boundaries and 
                current_size > max_chunk_size * 0.3 and  # è‡³å°‘è¾¾åˆ°30%çš„å¤§å°
                current_chunk.strip()):
                # ä¿å­˜å½“å‰å—
                chunks.append(current_chunk.strip())
                current_chunk = line_with_newline
                current_size = line_size
            else:
                # æ£€æŸ¥æ˜¯å¦ä¼šè¶…è¿‡æœ€å¤§å¤§å°
                if current_size + line_size > max_chunk_size and current_chunk.strip():
                    # å¯»æ‰¾æœ€è¿‘çš„é¢˜ç›®è¾¹ç•Œè¿›è¡Œåˆ†å‰²
                    split_point = self._find_best_split_point(current_chunk, question_boundaries, i, lines)
                    if split_point > 0:
                        chunks.append(current_chunk[:split_point].strip())
                        current_chunk = current_chunk[split_point:] + line_with_newline
                        current_size = len(current_chunk)
                    else:
                        # å¼ºåˆ¶åˆ†å‰²
                        chunks.append(current_chunk.strip())
                        current_chunk = line_with_newline
                        current_size = line_size
                else:
                    current_chunk += line_with_newline
                    current_size += line_size
        
        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _find_best_split_point(self, text: str, boundaries: List[int], current_line: int, all_lines: List[str]) -> int:
        """å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹"""
        # åœ¨æ–‡æœ¬ä¸­å¯»æ‰¾æœ€è¿‘çš„é¢˜ç›®å¼€å§‹ä½ç½®
        lines = text.split('\n')
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if re.match(r'ç¬¬\s*\d+\s*é¢˜', line):
                # æ‰¾åˆ°é¢˜ç›®å¼€å§‹ï¼Œè¿”å›è¯¥ä½ç½®
                return sum(len(lines[j] + '\n') for j in range(i))
        return 0
    
    def _split_by_size_fallback(self, pdf_text: str, max_chunk_size: int) -> List[str]:
        """å¤‡ç”¨åˆ†å‰²æ–¹æ³•ï¼ŒæŒ‰å¤§å°åˆ†å‰²"""
        chunks = []
        current_chunk = ""
        lines = pdf_text.split('\n')
        
        for line in lines:
            if len(current_chunk) + len(line) + 1 > max_chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = line
                else:
                    chunks.append(line[:max_chunk_size])
                    current_chunk = line[max_chunk_size:]
            else:
                current_chunk += "\n" + line if current_chunk else line
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def estimate_questions_in_text(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬ä¸­çš„é¢˜ç›®æ•°é‡
        åŸºäºpdfplumberæå–çš„ä¼˜åŒ–æ ¼å¼è¿›è¡Œä¼°ç®—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            ä¼°ç®—çš„é¢˜ç›®æ•°é‡
        """
        # åŸºäºæ–°çš„æ–‡æœ¬æ ¼å¼è¿›è¡Œä¼°ç®—
        question_count = 0
        
        # 1. ç›´æ¥è®¡ç®—é¢˜ç›®ç¼–å·
        question_pattern = r'ç¬¬\s*\d+\s*é¢˜'
        question_matches = re.findall(question_pattern, text)
        question_count = max(question_count, len(question_matches))
        
        # 2. è®¡ç®—é€‰é¡¹ç»„æ•°é‡ï¼ˆA. B. C. D. çš„ç»„åˆï¼‰
        option_groups = 0
        lines = text.split('\n')
        in_option_group = False
        option_count = 0
        
        for line in lines:
            line = line.strip()
            # æ£€æµ‹é€‰é¡¹å¼€å§‹
            if re.match(r'^[A-D]\s*[\.\)]\s*', line):
                if not in_option_group:
                    in_option_group = True
                    option_count = 1
                else:
                    option_count += 1
            else:
                # å¦‚æœä¹‹å‰æœ‰é€‰é¡¹ç»„ä¸”ç°åœ¨ç»“æŸäº†
                if in_option_group and option_count >= 2:  # è‡³å°‘2ä¸ªé€‰é¡¹æ‰ç®—ä¸€ä¸ªé¢˜ç›®
                    option_groups += 1
                in_option_group = False
                option_count = 0
        
        # å¤„ç†æœ€åä¸€ä¸ªé€‰é¡¹ç»„
        if in_option_group and option_count >= 2:
            option_groups += 1
        
        question_count = max(question_count, option_groups)
        
        # 3. è®¡ç®—ä»£ç å—æ•°é‡ï¼ˆä½œä¸ºç¼–ç¨‹é¢˜çš„æŒ‡æ ‡ï¼‰
        code_blocks = 0
        in_code_block = False
        for line in lines:
            line = line.strip()
            # æ£€æµ‹ä»£ç è¡Œï¼ˆæœ‰è¡Œå·çš„ä»£ç ï¼‰
            if re.match(r'^\d+\s+', line) and any(keyword in line for keyword in ['cin', 'cout', 'for', 'int', 'if', 'while']):
                if not in_code_block:
                    in_code_block = True
                    code_blocks += 1
            elif line and not re.match(r'^\d+\s+', line):
                in_code_block = False
        
        # 4. ç»¼åˆä¼°ç®—
        # ä¼˜å…ˆä½¿ç”¨é¢˜ç›®ç¼–å·ï¼Œå…¶æ¬¡ä½¿ç”¨é€‰é¡¹ç»„ï¼Œæœ€åä½¿ç”¨ä»£ç å—
        if question_count > 0:
            return question_count
        elif option_groups > 0:
            return option_groups
        elif code_blocks > 0:
            return code_blocks
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡å…¶ä»–æŒ‡æ ‡ä¼°ç®—
            fallback_indicators = [
                r'æ­£ç¡®ç­”æ¡ˆ\s*[A-D]',
                r'ç­”æ¡ˆ\s*[A-D]',
                r'\(\s*\d+\s*åˆ†\s*\)'
            ]
            
            count = 0
            for pattern in fallback_indicators:
                matches = re.findall(pattern, text, re.IGNORECASE)
                count += len(matches)
            
            return max(1, count)
    
    def process_pdf_text_stream(self, pdf_text: str, expected_questions: int = None) -> Generator[Dict[str, Any], None, None]:
        """æµå¼å¤„ç†PDFæ–‡æœ¬ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼ˆè·³è¿‡æ™ºèƒ½åˆ†å‰²ï¼‰"""
        try:
            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            yield {
                "type": "process_start",
                "message": "å¼€å§‹å¤„ç†PDFæ–‡æœ¬ï¼ˆç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼‰"
            }
            
            # ç›´æ¥ä½¿ç”¨æ•´ä¸ªPDFæ–‡æœ¬ï¼Œä¸è¿›è¡Œåˆ†å‰²
            yield {
                "type": "chunk_info",
                "message": f"PDFæ–‡æœ¬é•¿åº¦: {len(pdf_text)} å­—ç¬¦ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬",
                "chunk_count": 1
            }
            
            total_questions = 0
            all_questions = []
            
            # ç›´æ¥å¤„ç†æ•´ä¸ªPDFæ–‡æœ¬
            yield {
                "type": "chunk_start",
                "message": "å¼€å§‹å¤„ç†æ•´ä¸ªPDFæ–‡æœ¬",
                "chunk_index": 0,
                "chunk_size": len(pdf_text)
            }
            
            # ä¼°ç®—é¢˜ç›®æ•°é‡
            chunk_expected = self.estimate_questions_in_text(pdf_text)
            if expected_questions:
                chunk_expected = expected_questions
            
            # åˆ›å»ºprompt
            prompt = self.create_question_prompt(pdf_text, chunk_expected)
            
            # è°ƒç”¨æµå¼API
            stream_generator = self.call_api_stream(prompt)
            
            # è§£ææµå¼JSONå“åº”
            chunk_question_count = 0
            for question in self.parse_streaming_json(stream_generator):
                chunk_question_count += 1
                total_questions += 1
                all_questions.append(question)
                
                yield {
                    "type": "question",
                    "question": question,
                    "question_index": total_questions - 1,
                    "chunk_index": 0,
                    "message": f"âœ… ç¬¬ {total_questions} ä¸ªé¢˜ç›®æå–å®Œæˆ"
                }
            
            yield {
                "type": "chunk_complete",
                "message": f"æ•´ä¸ªPDFæ–‡æœ¬å¤„ç†å®Œæˆï¼Œæå–åˆ° {chunk_question_count} ä¸ªé¢˜ç›®",
                "chunk_index": 0,
                "chunk_questions": chunk_question_count
            }
            
            # æ£€æŸ¥é¢˜ç›®æ•°é‡æ˜¯å¦è¾¾åˆ°é¢„æœŸ
            warning_message = ""
            if expected_questions and total_questions < expected_questions:
                missing_count = expected_questions - total_questions
                warning_message = f"âš ï¸ è­¦å‘Šï¼šé¢„æœŸç”Ÿæˆ {expected_questions} ä¸ªé¢˜ç›®ï¼Œä½†åªæå–åˆ° {total_questions} ä¸ªï¼Œç¼ºå°‘ {missing_count} ä¸ªé¢˜ç›®ã€‚å¯èƒ½çš„åŸå› ï¼š1) PDFæ–‡æœ¬ä¸­å®é™…é¢˜ç›®æ•°é‡ä¸è¶³ï¼›2) LLMè¾“å‡ºè¢«æˆªæ–­ï¼›3) éƒ¨åˆ†é¢˜ç›®æ ¼å¼è¯†åˆ«å›°éš¾ã€‚"
                yield {
                    "type": "warning",
                    "message": warning_message,
                    "expected": expected_questions,
                    "actual": total_questions,
                    "missing": missing_count
                }
            
            # å‘é€å¤„ç†å®Œæˆæ¶ˆæ¯
            complete_message = f"ğŸ‰ å¤„ç†å®Œæˆï¼æ€»å…±æå–åˆ° {total_questions} ä¸ªé¢˜ç›®"
            if warning_message:
                complete_message += f"\n{warning_message}"
            
            yield {
                "type": "process_complete",
                "message": complete_message,
                "total_questions": total_questions,
                "chunk_count": 1,
                "expected_questions": expected_questions
            }
            
        except Exception as e:
            yield {
                "type": "process_error",
                "error": str(e),
                "message": f"âŒ å¤„ç†PDFæ–‡æœ¬å¤±è´¥: {str(e)}"
            }
    
    async def process_pdf_text_stream_async(self, pdf_text: str, expected_questions: int = None) -> AsyncGenerator[Dict[str, Any], None]:
        """å¼‚æ­¥æµå¼å¤„ç†PDFæ–‡æœ¬ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼ˆè·³è¿‡æ™ºèƒ½åˆ†å‰²ï¼‰"""
        try:
            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            yield {
                "type": "process_start",
                "message": "å¼€å§‹å¤„ç†PDFæ–‡æœ¬ï¼ˆç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼‰"
            }
            
            # ç›´æ¥ä½¿ç”¨æ•´ä¸ªPDFæ–‡æœ¬ï¼Œä¸è¿›è¡Œåˆ†å‰²
            yield {
                "type": "chunk_info",
                "message": f"PDFæ–‡æœ¬é•¿åº¦: {len(pdf_text)} å­—ç¬¦ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªæ–‡æœ¬",
                "chunk_count": 1
            }
            
            total_questions = 0
            all_questions = []
            
            # ç›´æ¥å¤„ç†æ•´ä¸ªPDFæ–‡æœ¬
            yield {
                "type": "chunk_start",
                "message": "å¼€å§‹å¤„ç†æ•´ä¸ªPDFæ–‡æœ¬",
                "chunk_index": 0,
                "chunk_size": len(pdf_text)
            }
            
            # ä¼°ç®—é¢˜ç›®æ•°é‡
            chunk_expected = self.estimate_questions_in_text(pdf_text)
            if expected_questions:
                chunk_expected = expected_questions
            
            # åˆ›å»ºprompt
            prompt = self.create_question_prompt(pdf_text, chunk_expected)
            
            # è°ƒç”¨å¼‚æ­¥æµå¼API
            stream_generator = self.call_api_stream_async(prompt)
            
            # è§£ææµå¼JSONå“åº”
            chunk_question_count = 0
            async for question in self.parse_streaming_json_async(stream_generator):
                chunk_question_count += 1
                total_questions += 1
                all_questions.append(question)
                
                yield {
                    "type": "question",
                    "question": question,
                    "question_index": total_questions - 1,
                    "chunk_index": 0,
                    "message": f"âœ… ç¬¬ {total_questions} ä¸ªé¢˜ç›®æå–å®Œæˆ"
                }
            
            yield {
                "type": "chunk_complete",
                "message": f"æ•´ä¸ªPDFæ–‡æœ¬å¤„ç†å®Œæˆï¼Œæå–åˆ° {chunk_question_count} ä¸ªé¢˜ç›®",
                "chunk_index": 0,
                "chunk_questions": chunk_question_count
            }
            
            # æ£€æŸ¥é¢˜ç›®æ•°é‡æ˜¯å¦è¾¾åˆ°é¢„æœŸ
            warning_message = ""
            if expected_questions and total_questions < expected_questions:
                missing_count = expected_questions - total_questions
                warning_message = f"âš ï¸ è­¦å‘Šï¼šé¢„æœŸç”Ÿæˆ {expected_questions} ä¸ªé¢˜ç›®ï¼Œä½†åªæå–åˆ° {total_questions} ä¸ªï¼Œç¼ºå°‘ {missing_count} ä¸ªé¢˜ç›®ã€‚å¯èƒ½çš„åŸå› ï¼š1) PDFæ–‡æœ¬ä¸­å®é™…é¢˜ç›®æ•°é‡ä¸è¶³ï¼›2) LLMè¾“å‡ºè¢«æˆªæ–­ï¼›3) éƒ¨åˆ†é¢˜ç›®æ ¼å¼è¯†åˆ«å›°éš¾ã€‚"
                yield {
                    "type": "warning",
                    "message": warning_message,
                    "expected": expected_questions,
                    "actual": total_questions,
                    "missing": missing_count
                }
            
            # å‘é€å¤„ç†å®Œæˆæ¶ˆæ¯
            complete_message = f"ğŸ‰ å¤„ç†å®Œæˆï¼æ€»å…±æå–åˆ° {total_questions} ä¸ªé¢˜ç›®"
            if warning_message:
                complete_message += f"\n{warning_message}"
            
            yield {
                "type": "process_complete",
                "message": complete_message,
                "total_questions": total_questions,
                "chunk_count": 1,
                "expected_questions": expected_questions
            }
            
        except Exception as e:
            yield {
                "type": "process_error",
                "error": str(e),
                "message": f"âŒ å¤„ç†PDFæ–‡æœ¬å¤±è´¥: {str(e)}"
            }
